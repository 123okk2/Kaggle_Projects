{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nprint(train_df.head(5))\nprint(test_df.head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove NA\ntrain_df['keyword'].fillna(\"None\", inplace = True)\ntrain_df['location'].fillna(\"None\", inplace = True)\ntest_df['keyword'].fillna(\"None\", inplace = True)\ntest_df['location'].fillna(\"None\", inplace = True)\nprint(train_df.head(5))\nprint(test_df.head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport re\n\ntrain_keyword = train_df[\"keyword\"].values\ntrain_location = train_df[\"location\"].values\ntrain_text = train_df[\"text\"].values\n\n#remove useless characters\nfor i in range(len(train_keyword)) :\n    train_keyword[i] = re.sub('[^0-9a-zA-Z ]', '', train_keyword[i])\n    train_location[i] = re.sub('[^0-9a-zA-Z ]', '', train_location[i])\n    train_text[i] = re.sub('[^0-9a-zA-Z ]', '', train_text[i])\n\n#make String data to numeric data\ntfidf_keyword = TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range = (1, 2))\ntfidf_keyword.fit(train_keyword)\ntrain_keyword = tfidf_keyword.transform(train_keyword).toarray()\n\ntfidf_location = TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range = (1, 2))\ntfidf_location.fit(train_location)\ntrain_location = tfidf_keyword.transform(train_location).toarray()\n\ntfidf_text = TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range = (1, 2))\ntfidf_text.fit(train_text)\ntrain_text = tfidf_keyword.transform(train_text).toarray()\n\ntrain_x = []\ntrain_y = train_df[\"target\"].values\n\nfor i in range(len(train_y)) :\n    train_x.append([train_keyword[i], train_location[i], train_text[i]])\n    \ntrain_x = np.array(train_x, dtype=object)\nprint(train_x.shape) #7613, 3, 222\n#reshape for sklearn : sklearn allows 2D shape only\ntrain_x = train_x.reshape((train_x.shape[0], -1))\nprint(train_x.shape) #7613, 666","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\n\nlg = LogisticRegression(random_state=0)\n\n#search the best parameter\nparams = { 'C': list(np.arange(1,10,0.1)) }\n\n#train\ngrid_cv = GridSearchCV(lg , param_grid=params , cv=5 ,scoring='accuracy', verbose=1 )\ngrid_cv.fit(train_x , train_y)\n\n#check best parameter and accuracy score\nprint(grid_cv.best_params_ , round(grid_cv.best_score_,4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_keyword = test_df[\"keyword\"].values\ntest_location = test_df[\"location\"].values\ntest_text = test_df[\"text\"].values\n\n#remove useless characters\nfor i in range(len(test_keyword)) :\n    test_keyword[i] = re.sub('[^0-9a-zA-Z ]', '', test_keyword[i])\n    test_location[i] = re.sub('[^0-9a-zA-Z ]', '', test_location[i])\n    test_text[i] = re.sub('[^0-9a-zA-Z ]', '', test_text[i])\n\n#make String data to numeric data\ntest_keyword = tfidf_keyword.transform(test_keyword).toarray()\n\ntest_location = tfidf_keyword.transform(test_location).toarray()\n\ntest_text = tfidf_keyword.transform(test_text).toarray()\n\ntest_x = []\n\nfor i in range(len(test_keyword)) :\n    test_x.append([test_keyword[i], test_location[i], test_text[i]])\n    \ntest_x = np.array(test_x, dtype=object)\nprint(test_x.shape) #3263, 3, 222\n#reshape for sklearn : sklearn allows 2D shape only\ntest_x = test_x.reshape((test_x.shape[0], -1))\nprint(test_x.shape) #3263, 666","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = grid_cv.predict(test_x)\npred = pd.Series(pred, name = 'target')\ntarget_id = test_df[\"id\"]\nsave_data = pd.concat([target_id, pred], axis = 1)\nprint(save_data.head(5))\n\nsave_data.to_csv(\"result.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}