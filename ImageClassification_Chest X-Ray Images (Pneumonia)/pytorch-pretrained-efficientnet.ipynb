{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"\"\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocess data using ImageFolder and DataLoader","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nnum_classes = 2\ntransform = transforms.Compose([\n    transforms.Resize([224, 224]),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = ImageFolder(root = '../input/chest-xray-pneumonia/chest_xray/train', transform = transform)\ntrain_dataset = DataLoader(train_dataset, batch_size = 16, shuffle = True, num_workers = 2)\n\ntest_dataset = ImageFolder(root = '../input/chest-xray-pneumonia/chest_xray/test', transform = transform)\ntest_dataset = DataLoader(test_dataset, batch_size = 16, shuffle = True, num_workers = 2)\n\nval_dataset = ImageFolder(root = '../input/chest-xray-pneumonia/chest_xray/val', transform = transform)\nval_dataset = DataLoader(val_dataset, batch_size = 16, shuffle = True, num_workers = 2)","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Construct model by using resnet34 from torchvision","metadata":{}},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"scrolled":true,"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (1.19.5)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=77c02f370bd4f73c7ea012edb731dee17f853c82ad2c33e619d2a2b2e59e454b\n  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchsummary import summary\nfrom efficientnet_pytorch import EfficientNet\nfrom torch.optim import Adam, lr_scheduler\n\ndevice = torch.device('cuda' if torch.cuda.is_available else 'cpu')\nprint(device)\n\nmodel = EfficientNet.from_pretrained('efficientnet-b7', num_classes = num_classes).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = Adam(model.parameters(), lr = 0.001)\nscheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma = 0.1)","metadata":{"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"cuda\nLoaded pretrained weights for efficientnet-b7\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nn_epoch = 20\n\nfor epoch in range(n_epoch) :\n    #train\n    epoch_acc = 0\n    epoch_loss = 0\n    model.train()\n    for x, y in tqdm(train_dataset) :\n        x = x.to(device)\n        y = y.to(device)\n        \n        predict = model(x)\n        loss = criterion(predict, y)\n        epoch_loss += loss\n        correct_pred = torch.argmax(predict, 1) == y\n        correct_pred = correct_pred.sum()\n        epoch_acc += correct_pred / len(y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    epoch_acc = epoch_acc / len(train_dataset)\n    epoch_loss = epoch_loss / len(train_dataset)\n    \n    #test\n    epoch_val_acc = 0\n    epoch_val_loss = 0\n    model.eval()\n    with torch.no_grad() :\n        for x, y in tqdm(test_dataset) :\n            x = x.to(device)\n            y = y.to(device)\n            \n            predict = model(x)\n            loss = criterion(predict, y)\n            epoch_val_loss += loss\n            correct_pred = torch.argmax(predict, 1) == y\n            correct_pred = correct_pred.sum()\n            epoch_val_acc += correct_pred / len(y)\n            \n    epoch_val_acc = epoch_val_acc / len(test_dataset)\n    epoch_val_loss = epoch_val_loss / len(test_dataset)\n    \n    print('{0} / {1} : train_loss : {2:.4f}, train_acc : {3:.4f}, val_loss : {4:.4f}, val_acc : {5:.4f}'.format(epoch+1, n_epoch, epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc))\n    \n    if (epoch_val_acc > 0.90 or epoch_acc > 0.98) :\n        print('early stop')\n        break\n    scheduler.step()","metadata":{"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"100%|██████████| 326/326 [03:23<00:00,  1.60it/s]\n100%|██████████| 39/39 [00:10<00:00,  3.58it/s]","output_type":"stream"},{"name":"stdout","text":"1 / 20 : train_loss : 0.0334, train_acc : 0.9889, val_loss : 0.8310, val_acc : 0.8446\nearly stop\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Check the performance using valid_dataset","metadata":{}},{"cell_type":"code","source":"model.eval()\n\nvalid_acc = 0\nvalid_loss = 0\nwith torch.no_grad() :\n    for x, y in tqdm(val_dataset) :\n        x = x.to(device)\n        y = y.to(device)\n\n        predict = model(x)\n        loss = criterion(predict, y)\n        valid_loss += loss\n        correct_pred = torch.argmax(predict, 1) == y\n        correct_pred = correct_pred.sum()\n        valid_acc += correct_pred / len(y)\n    \n    valid_acc = valid_acc / len(val_dataset)\n    valid_loss = valid_loss / len(val_dataset)\n\nprint('valid_loss : {0:.4f}, valid_acc : {1:.4f}'.format(valid_loss, valid_acc))","metadata":{"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  1.88it/s]","output_type":"stream"},{"name":"stdout","text":"valid_loss : 0.0318, valid_acc : 1.0000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}