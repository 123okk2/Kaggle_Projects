{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ntrain_data.head()","metadata":{"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#check null\ntrain_data.info()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"#convert numeric data to range data\ndef divide_convert(age, num = 10) :\n    return int(age/num)\n\ntrain_data[\"Age\"] = train_data[\"Age\"].apply(lambda x : divide_convert(x))\ntrain_data[\"SkinThickness\"] = train_data[\"SkinThickness\"].apply(lambda x : divide_convert(x))\n\ntrain_data.head()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72              3        0  33.6   \n1            1       85             66              2        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66              2       94  28.1   \n4            0      137             40              3      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627    5        1  \n1                     0.351    3        0  \n2                     0.672    3        1  \n3                     0.167    2        0  \n4                     2.288    3        1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>3</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>2</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>2</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>3</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#use last 20 datas for validate\nx = train_data.iloc[:-20, :-1].to_numpy()\nx = torch.FloatTensor(x).to(device)\ny = train_data.iloc[:-20, -1].to_numpy()\ny = torch.FloatTensor(y).to(device).reshape(-1, 1)\n\nvalid_x = train_data.iloc[-20:, :-1].to_numpy()\nvalid_x = torch.FloatTensor(valid_x).to(device)\nvalid_y = train_data.iloc[-20:, -1].to_numpy()\nvalid_y = torch.FloatTensor(valid_y).to(device).reshape(-1, 1)\n\nprint(x.shape, y.shape)\nprint(valid_x.shape, valid_y.shape)","metadata":{"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"torch.Size([748, 8]) torch.Size([748, 1])\ntorch.Size([20, 8]) torch.Size([20, 1])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass BinaryDnnModel(nn.Module) :\n    def __init__(self, features) :\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(features, features*2),\n            nn.LeakyReLU(),\n            nn.Linear(features*2, features),\n            nn.LeakyReLU(),\n            nn.Linear(features, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x) :\n        return self.model(x)\n\nmodel = BinaryDnnModel(x.shape[1]).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)","metadata":{"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"n_epoch = 500\nmodel.train()\n\nfor epoch in range(n_epoch) :\n    predict = model(x)\n    \n    loss = F.binary_cross_entropy(predict, y)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    predict = predict >= torch.FloatTensor([0.5]).to(device)\n    predict = predict.float()\n    \n    acc = (predict == y)\n    acc = acc.sum()\n    acc = acc / x.shape[0]\n    \n    print('Epoch : {}/{},   loss : {:.5f},    acc : {:.5f}'.format(epoch+1, n_epoch, loss.item(), acc))","metadata":{"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Epoch : 1/500,   loss : 1.70892,    acc : 0.55481\nEpoch : 2/500,   loss : 22.77833,    acc : 0.47727\nEpoch : 3/500,   loss : 28.51794,    acc : 0.34492\nEpoch : 4/500,   loss : 3.26666,    acc : 0.59759\nEpoch : 5/500,   loss : 0.67834,    acc : 0.66310\nEpoch : 6/500,   loss : 0.65696,    acc : 0.65642\nEpoch : 7/500,   loss : 2.28704,    acc : 0.34492\nEpoch : 8/500,   loss : 2.01410,    acc : 0.65508\nEpoch : 9/500,   loss : 2.36542,    acc : 0.65508\nEpoch : 10/500,   loss : 1.26757,    acc : 0.65508\nEpoch : 11/500,   loss : 0.67350,    acc : 0.61898\nEpoch : 12/500,   loss : 1.07750,    acc : 0.35294\nEpoch : 13/500,   loss : 0.91165,    acc : 0.36497\nEpoch : 14/500,   loss : 0.65153,    acc : 0.66979\nEpoch : 15/500,   loss : 0.63855,    acc : 0.66310\nEpoch : 16/500,   loss : 0.63597,    acc : 0.65374\nEpoch : 17/500,   loss : 0.63897,    acc : 0.65508\nEpoch : 18/500,   loss : 0.64423,    acc : 0.65508\nEpoch : 19/500,   loss : 0.64793,    acc : 0.65508\nEpoch : 20/500,   loss : 0.64783,    acc : 0.65508\nEpoch : 21/500,   loss : 0.64384,    acc : 0.65508\nEpoch : 22/500,   loss : 0.63759,    acc : 0.65508\nEpoch : 23/500,   loss : 0.63174,    acc : 0.65508\nEpoch : 24/500,   loss : 0.62852,    acc : 0.65374\nEpoch : 25/500,   loss : 0.62865,    acc : 0.65642\nEpoch : 26/500,   loss : 0.63099,    acc : 0.67647\nEpoch : 27/500,   loss : 0.63331,    acc : 0.68583\nEpoch : 28/500,   loss : 0.63363,    acc : 0.68984\nEpoch : 29/500,   loss : 0.63143,    acc : 0.68717\nEpoch : 30/500,   loss : 0.62782,    acc : 0.67781\nEpoch : 31/500,   loss : 0.62456,    acc : 0.65909\nEpoch : 32/500,   loss : 0.62292,    acc : 0.65374\nEpoch : 33/500,   loss : 0.62297,    acc : 0.65374\nEpoch : 34/500,   loss : 0.62378,    acc : 0.65508\nEpoch : 35/500,   loss : 0.62414,    acc : 0.65508\nEpoch : 36/500,   loss : 0.62334,    acc : 0.65508\nEpoch : 37/500,   loss : 0.62143,    acc : 0.65508\nEpoch : 38/500,   loss : 0.61916,    acc : 0.65374\nEpoch : 39/500,   loss : 0.61739,    acc : 0.65642\nEpoch : 40/500,   loss : 0.61658,    acc : 0.65775\nEpoch : 41/500,   loss : 0.61649,    acc : 0.67112\nEpoch : 42/500,   loss : 0.61644,    acc : 0.67914\nEpoch : 43/500,   loss : 0.61578,    acc : 0.68048\nEpoch : 44/500,   loss : 0.61441,    acc : 0.67647\nEpoch : 45/500,   loss : 0.61276,    acc : 0.67380\nEpoch : 46/500,   loss : 0.61139,    acc : 0.66043\nEpoch : 47/500,   loss : 0.61056,    acc : 0.65775\nEpoch : 48/500,   loss : 0.61009,    acc : 0.65642\nEpoch : 49/500,   loss : 0.60959,    acc : 0.65642\nEpoch : 50/500,   loss : 0.60875,    acc : 0.65642\nEpoch : 51/500,   loss : 0.60756,    acc : 0.65642\nEpoch : 52/500,   loss : 0.60628,    acc : 0.65909\nEpoch : 53/500,   loss : 0.60527,    acc : 0.66444\nEpoch : 54/500,   loss : 0.60404,    acc : 0.67112\nEpoch : 55/500,   loss : 0.60312,    acc : 0.67781\nEpoch : 56/500,   loss : 0.60238,    acc : 0.67647\nEpoch : 57/500,   loss : 0.60162,    acc : 0.67647\nEpoch : 58/500,   loss : 0.60084,    acc : 0.67781\nEpoch : 59/500,   loss : 0.60138,    acc : 0.67781\nEpoch : 60/500,   loss : 0.60002,    acc : 0.66979\nEpoch : 61/500,   loss : 0.59956,    acc : 0.66310\nEpoch : 62/500,   loss : 0.59887,    acc : 0.66310\nEpoch : 63/500,   loss : 0.59791,    acc : 0.66845\nEpoch : 64/500,   loss : 0.59687,    acc : 0.67380\nEpoch : 65/500,   loss : 0.59599,    acc : 0.68048\nEpoch : 66/500,   loss : 0.59534,    acc : 0.68449\nEpoch : 67/500,   loss : 0.59467,    acc : 0.69251\nEpoch : 68/500,   loss : 0.59377,    acc : 0.69118\nEpoch : 69/500,   loss : 0.59273,    acc : 0.68717\nEpoch : 70/500,   loss : 0.59177,    acc : 0.68316\nEpoch : 71/500,   loss : 0.59098,    acc : 0.68182\nEpoch : 72/500,   loss : 0.59021,    acc : 0.68048\nEpoch : 73/500,   loss : 0.58931,    acc : 0.68316\nEpoch : 74/500,   loss : 0.58829,    acc : 0.68583\nEpoch : 75/500,   loss : 0.58731,    acc : 0.68850\nEpoch : 76/500,   loss : 0.58645,    acc : 0.69920\nEpoch : 77/500,   loss : 0.58560,    acc : 0.70722\nEpoch : 78/500,   loss : 0.58465,    acc : 0.70989\nEpoch : 79/500,   loss : 0.58362,    acc : 0.70455\nEpoch : 80/500,   loss : 0.58265,    acc : 0.69786\nEpoch : 81/500,   loss : 0.58175,    acc : 0.69652\nEpoch : 82/500,   loss : 0.58081,    acc : 0.69652\nEpoch : 83/500,   loss : 0.57980,    acc : 0.70321\nEpoch : 84/500,   loss : 0.57877,    acc : 0.70856\nEpoch : 85/500,   loss : 0.57781,    acc : 0.70989\nEpoch : 86/500,   loss : 0.57685,    acc : 0.71791\nEpoch : 87/500,   loss : 0.57583,    acc : 0.71925\nEpoch : 88/500,   loss : 0.57478,    acc : 0.71658\nEpoch : 89/500,   loss : 0.57376,    acc : 0.71257\nEpoch : 90/500,   loss : 0.57275,    acc : 0.71390\nEpoch : 91/500,   loss : 0.57171,    acc : 0.71524\nEpoch : 92/500,   loss : 0.57064,    acc : 0.71925\nEpoch : 93/500,   loss : 0.56958,    acc : 0.72193\nEpoch : 94/500,   loss : 0.56854,    acc : 0.72594\nEpoch : 95/500,   loss : 0.56746,    acc : 0.72861\nEpoch : 96/500,   loss : 0.56635,    acc : 0.72727\nEpoch : 97/500,   loss : 0.56526,    acc : 0.72460\nEpoch : 98/500,   loss : 0.56417,    acc : 0.72727\nEpoch : 99/500,   loss : 0.56305,    acc : 0.73128\nEpoch : 100/500,   loss : 0.56192,    acc : 0.73797\nEpoch : 101/500,   loss : 0.56079,    acc : 0.74332\nEpoch : 102/500,   loss : 0.55966,    acc : 0.74332\nEpoch : 103/500,   loss : 0.55850,    acc : 0.74465\nEpoch : 104/500,   loss : 0.55734,    acc : 0.74332\nEpoch : 105/500,   loss : 0.55619,    acc : 0.74465\nEpoch : 106/500,   loss : 0.55502,    acc : 0.74599\nEpoch : 107/500,   loss : 0.55384,    acc : 0.74465\nEpoch : 108/500,   loss : 0.55267,    acc : 0.74599\nEpoch : 109/500,   loss : 0.55150,    acc : 0.74599\nEpoch : 110/500,   loss : 0.55031,    acc : 0.74465\nEpoch : 111/500,   loss : 0.54913,    acc : 0.74599\nEpoch : 112/500,   loss : 0.54796,    acc : 0.74599\nEpoch : 113/500,   loss : 0.54678,    acc : 0.74599\nEpoch : 114/500,   loss : 0.54560,    acc : 0.74866\nEpoch : 115/500,   loss : 0.54444,    acc : 0.75267\nEpoch : 116/500,   loss : 0.54328,    acc : 0.75401\nEpoch : 117/500,   loss : 0.54213,    acc : 0.75668\nEpoch : 118/500,   loss : 0.54099,    acc : 0.75668\nEpoch : 119/500,   loss : 0.53986,    acc : 0.75802\nEpoch : 120/500,   loss : 0.53875,    acc : 0.75936\nEpoch : 121/500,   loss : 0.53766,    acc : 0.76070\nEpoch : 122/500,   loss : 0.53658,    acc : 0.76337\nEpoch : 123/500,   loss : 0.53534,    acc : 0.76203\nEpoch : 124/500,   loss : 0.54139,    acc : 0.75802\nEpoch : 125/500,   loss : 0.53635,    acc : 0.75000\nEpoch : 126/500,   loss : 0.53634,    acc : 0.75000\nEpoch : 127/500,   loss : 0.53206,    acc : 0.76337\nEpoch : 128/500,   loss : 0.53407,    acc : 0.75802\nEpoch : 129/500,   loss : 0.53244,    acc : 0.76070\nEpoch : 130/500,   loss : 0.52991,    acc : 0.75936\nEpoch : 131/500,   loss : 0.53158,    acc : 0.75000\nEpoch : 132/500,   loss : 0.52859,    acc : 0.75936\nEpoch : 133/500,   loss : 0.52836,    acc : 0.75668\nEpoch : 134/500,   loss : 0.52825,    acc : 0.75267\nEpoch : 135/500,   loss : 0.52578,    acc : 0.76070\nEpoch : 136/500,   loss : 0.52675,    acc : 0.75802\nEpoch : 137/500,   loss : 0.52470,    acc : 0.75936\nEpoch : 138/500,   loss : 0.52446,    acc : 0.75267\nEpoch : 139/500,   loss : 0.52400,    acc : 0.75000\nEpoch : 140/500,   loss : 0.52251,    acc : 0.75802\nEpoch : 141/500,   loss : 0.52288,    acc : 0.75936\nEpoch : 142/500,   loss : 0.52116,    acc : 0.75668\nEpoch : 143/500,   loss : 0.52133,    acc : 0.75535\nEpoch : 144/500,   loss : 0.52019,    acc : 0.75267\nEpoch : 145/500,   loss : 0.51972,    acc : 0.75936\nEpoch : 146/500,   loss : 0.51927,    acc : 0.75802\nEpoch : 147/500,   loss : 0.51831,    acc : 0.75668\nEpoch : 148/500,   loss : 0.51824,    acc : 0.75401\nEpoch : 149/500,   loss : 0.51715,    acc : 0.75668\nEpoch : 150/500,   loss : 0.51709,    acc : 0.75936\nEpoch : 151/500,   loss : 0.51614,    acc : 0.75535\nEpoch : 152/500,   loss : 0.51589,    acc : 0.75936\nEpoch : 153/500,   loss : 0.51517,    acc : 0.76070\nEpoch : 154/500,   loss : 0.51471,    acc : 0.75668\nEpoch : 155/500,   loss : 0.51419,    acc : 0.75668\nEpoch : 156/500,   loss : 0.51359,    acc : 0.76070\nEpoch : 157/500,   loss : 0.51320,    acc : 0.76070\nEpoch : 158/500,   loss : 0.51254,    acc : 0.75936\nEpoch : 159/500,   loss : 0.51220,    acc : 0.75936\nEpoch : 160/500,   loss : 0.51154,    acc : 0.76070\nEpoch : 161/500,   loss : 0.51122,    acc : 0.76203\nEpoch : 162/500,   loss : 0.51059,    acc : 0.76070\nEpoch : 163/500,   loss : 0.51026,    acc : 0.75936\nEpoch : 164/500,   loss : 0.50968,    acc : 0.75936\nEpoch : 165/500,   loss : 0.50933,    acc : 0.76203\nEpoch : 166/500,   loss : 0.50881,    acc : 0.76203\nEpoch : 167/500,   loss : 0.50843,    acc : 0.76070\nEpoch : 168/500,   loss : 0.50796,    acc : 0.75936\nEpoch : 169/500,   loss : 0.50757,    acc : 0.76471\nEpoch : 170/500,   loss : 0.50713,    acc : 0.76337\nEpoch : 171/500,   loss : 0.50674,    acc : 0.76070\nEpoch : 172/500,   loss : 0.50633,    acc : 0.75936\nEpoch : 173/500,   loss : 0.50594,    acc : 0.76337\nEpoch : 174/500,   loss : 0.50556,    acc : 0.76337\nEpoch : 175/500,   loss : 0.50518,    acc : 0.76070\nEpoch : 176/500,   loss : 0.50482,    acc : 0.76337\nEpoch : 177/500,   loss : 0.50445,    acc : 0.76203\nEpoch : 178/500,   loss : 0.50410,    acc : 0.76203\nEpoch : 179/500,   loss : 0.50374,    acc : 0.76203\nEpoch : 180/500,   loss : 0.50340,    acc : 0.76070\nEpoch : 181/500,   loss : 0.50305,    acc : 0.75936\nEpoch : 182/500,   loss : 0.50273,    acc : 0.75936\nEpoch : 183/500,   loss : 0.50240,    acc : 0.75802\nEpoch : 184/500,   loss : 0.50208,    acc : 0.75802\nEpoch : 185/500,   loss : 0.50176,    acc : 0.75802\nEpoch : 186/500,   loss : 0.50145,    acc : 0.75802\nEpoch : 187/500,   loss : 0.50115,    acc : 0.75668\nEpoch : 188/500,   loss : 0.50085,    acc : 0.75668\nEpoch : 189/500,   loss : 0.50056,    acc : 0.75936\nEpoch : 190/500,   loss : 0.50027,    acc : 0.75936\nEpoch : 191/500,   loss : 0.49998,    acc : 0.75668\nEpoch : 192/500,   loss : 0.49970,    acc : 0.75802\nEpoch : 193/500,   loss : 0.49943,    acc : 0.75936\nEpoch : 194/500,   loss : 0.49915,    acc : 0.75936\nEpoch : 195/500,   loss : 0.49889,    acc : 0.75936\nEpoch : 196/500,   loss : 0.49862,    acc : 0.76203\nEpoch : 197/500,   loss : 0.49836,    acc : 0.76203\nEpoch : 198/500,   loss : 0.49810,    acc : 0.76203\nEpoch : 199/500,   loss : 0.49785,    acc : 0.76070\nEpoch : 200/500,   loss : 0.49760,    acc : 0.76070\nEpoch : 201/500,   loss : 0.49736,    acc : 0.76070\nEpoch : 202/500,   loss : 0.49711,    acc : 0.76070\nEpoch : 203/500,   loss : 0.49687,    acc : 0.76070\nEpoch : 204/500,   loss : 0.49663,    acc : 0.76203\nEpoch : 205/500,   loss : 0.49640,    acc : 0.76203\nEpoch : 206/500,   loss : 0.49617,    acc : 0.76070\nEpoch : 207/500,   loss : 0.49594,    acc : 0.76070\nEpoch : 208/500,   loss : 0.49571,    acc : 0.76203\nEpoch : 209/500,   loss : 0.49549,    acc : 0.76203\nEpoch : 210/500,   loss : 0.49527,    acc : 0.76203\nEpoch : 211/500,   loss : 0.49505,    acc : 0.76337\nEpoch : 212/500,   loss : 0.49483,    acc : 0.76337\nEpoch : 213/500,   loss : 0.49462,    acc : 0.76337\nEpoch : 214/500,   loss : 0.49440,    acc : 0.76337\nEpoch : 215/500,   loss : 0.49419,    acc : 0.76471\nEpoch : 216/500,   loss : 0.49399,    acc : 0.76604\nEpoch : 217/500,   loss : 0.49378,    acc : 0.76604\nEpoch : 218/500,   loss : 0.49358,    acc : 0.76604\nEpoch : 219/500,   loss : 0.49337,    acc : 0.76738\nEpoch : 220/500,   loss : 0.49317,    acc : 0.76604\nEpoch : 221/500,   loss : 0.49297,    acc : 0.76604\nEpoch : 222/500,   loss : 0.49278,    acc : 0.76872\nEpoch : 223/500,   loss : 0.49258,    acc : 0.76872\nEpoch : 224/500,   loss : 0.49239,    acc : 0.76872\nEpoch : 225/500,   loss : 0.49220,    acc : 0.76738\nEpoch : 226/500,   loss : 0.49200,    acc : 0.76738\nEpoch : 227/500,   loss : 0.49182,    acc : 0.76738\nEpoch : 228/500,   loss : 0.49163,    acc : 0.76738\nEpoch : 229/500,   loss : 0.49144,    acc : 0.76738\nEpoch : 230/500,   loss : 0.49126,    acc : 0.76738\nEpoch : 231/500,   loss : 0.49108,    acc : 0.76738\nEpoch : 232/500,   loss : 0.49090,    acc : 0.76738\nEpoch : 233/500,   loss : 0.49072,    acc : 0.76738\nEpoch : 234/500,   loss : 0.49054,    acc : 0.76738\nEpoch : 235/500,   loss : 0.49036,    acc : 0.76738\nEpoch : 236/500,   loss : 0.49018,    acc : 0.76738\nEpoch : 237/500,   loss : 0.49001,    acc : 0.76872\nEpoch : 238/500,   loss : 0.48983,    acc : 0.76738\nEpoch : 239/500,   loss : 0.48966,    acc : 0.76872\nEpoch : 240/500,   loss : 0.48949,    acc : 0.76872\nEpoch : 241/500,   loss : 0.48932,    acc : 0.76872\nEpoch : 242/500,   loss : 0.48915,    acc : 0.76872\nEpoch : 243/500,   loss : 0.48899,    acc : 0.76872\nEpoch : 244/500,   loss : 0.48882,    acc : 0.76872\nEpoch : 245/500,   loss : 0.48865,    acc : 0.77005\nEpoch : 246/500,   loss : 0.48849,    acc : 0.77005\nEpoch : 247/500,   loss : 0.48833,    acc : 0.76872\nEpoch : 248/500,   loss : 0.48817,    acc : 0.77005\nEpoch : 249/500,   loss : 0.48801,    acc : 0.77005\nEpoch : 250/500,   loss : 0.48785,    acc : 0.77005\nEpoch : 251/500,   loss : 0.48769,    acc : 0.77005\nEpoch : 252/500,   loss : 0.48754,    acc : 0.77005\nEpoch : 253/500,   loss : 0.48738,    acc : 0.77005\nEpoch : 254/500,   loss : 0.48723,    acc : 0.77005\nEpoch : 255/500,   loss : 0.48707,    acc : 0.77139\nEpoch : 256/500,   loss : 0.48692,    acc : 0.77139\nEpoch : 257/500,   loss : 0.48677,    acc : 0.77139\nEpoch : 258/500,   loss : 0.48662,    acc : 0.77139\nEpoch : 259/500,   loss : 0.48647,    acc : 0.77005\nEpoch : 260/500,   loss : 0.48632,    acc : 0.77005\nEpoch : 261/500,   loss : 0.48617,    acc : 0.76738\nEpoch : 262/500,   loss : 0.48603,    acc : 0.76738\nEpoch : 263/500,   loss : 0.48588,    acc : 0.76738\nEpoch : 264/500,   loss : 0.48574,    acc : 0.76738\nEpoch : 265/500,   loss : 0.48559,    acc : 0.76738\nEpoch : 266/500,   loss : 0.48545,    acc : 0.76738\nEpoch : 267/500,   loss : 0.48531,    acc : 0.76738\nEpoch : 268/500,   loss : 0.48517,    acc : 0.76738\nEpoch : 269/500,   loss : 0.48503,    acc : 0.76738\nEpoch : 270/500,   loss : 0.48489,    acc : 0.76738\nEpoch : 271/500,   loss : 0.48475,    acc : 0.76738\nEpoch : 272/500,   loss : 0.48461,    acc : 0.76738\nEpoch : 273/500,   loss : 0.48447,    acc : 0.76738\nEpoch : 274/500,   loss : 0.48433,    acc : 0.76738\nEpoch : 275/500,   loss : 0.48420,    acc : 0.76738\nEpoch : 276/500,   loss : 0.48406,    acc : 0.76738\nEpoch : 277/500,   loss : 0.48393,    acc : 0.76738\nEpoch : 278/500,   loss : 0.48380,    acc : 0.76738\nEpoch : 279/500,   loss : 0.48366,    acc : 0.76738\nEpoch : 280/500,   loss : 0.48353,    acc : 0.76604\nEpoch : 281/500,   loss : 0.48340,    acc : 0.76604\nEpoch : 282/500,   loss : 0.48327,    acc : 0.76604\nEpoch : 283/500,   loss : 0.48314,    acc : 0.76738\nEpoch : 284/500,   loss : 0.48301,    acc : 0.76738\nEpoch : 285/500,   loss : 0.48288,    acc : 0.76738\nEpoch : 286/500,   loss : 0.48276,    acc : 0.76738\nEpoch : 287/500,   loss : 0.48263,    acc : 0.76872\nEpoch : 288/500,   loss : 0.48250,    acc : 0.76872\nEpoch : 289/500,   loss : 0.48238,    acc : 0.76872\nEpoch : 290/500,   loss : 0.48226,    acc : 0.76872\nEpoch : 291/500,   loss : 0.48213,    acc : 0.76872\nEpoch : 292/500,   loss : 0.48201,    acc : 0.76738\nEpoch : 293/500,   loss : 0.48189,    acc : 0.76738\nEpoch : 294/500,   loss : 0.48177,    acc : 0.76738\nEpoch : 295/500,   loss : 0.48165,    acc : 0.76738\nEpoch : 296/500,   loss : 0.48153,    acc : 0.76738\nEpoch : 297/500,   loss : 0.48141,    acc : 0.77005\nEpoch : 298/500,   loss : 0.48130,    acc : 0.77005\nEpoch : 299/500,   loss : 0.48119,    acc : 0.77005\nEpoch : 300/500,   loss : 0.48107,    acc : 0.77005\nEpoch : 301/500,   loss : 0.48096,    acc : 0.77005\nEpoch : 302/500,   loss : 0.48085,    acc : 0.77005\nEpoch : 303/500,   loss : 0.48074,    acc : 0.77005\nEpoch : 304/500,   loss : 0.48063,    acc : 0.77005\nEpoch : 305/500,   loss : 0.48052,    acc : 0.77005\nEpoch : 306/500,   loss : 0.48041,    acc : 0.77005\nEpoch : 307/500,   loss : 0.48030,    acc : 0.77005\nEpoch : 308/500,   loss : 0.48020,    acc : 0.77005\nEpoch : 309/500,   loss : 0.48009,    acc : 0.77005\nEpoch : 310/500,   loss : 0.47999,    acc : 0.76872\nEpoch : 311/500,   loss : 0.47989,    acc : 0.77273\nEpoch : 312/500,   loss : 0.47978,    acc : 0.77139\nEpoch : 313/500,   loss : 0.47968,    acc : 0.77139\nEpoch : 314/500,   loss : 0.47958,    acc : 0.77273\nEpoch : 315/500,   loss : 0.47948,    acc : 0.77139\nEpoch : 316/500,   loss : 0.47938,    acc : 0.77406\nEpoch : 317/500,   loss : 0.47929,    acc : 0.77273\nEpoch : 318/500,   loss : 0.47919,    acc : 0.77406\nEpoch : 319/500,   loss : 0.47910,    acc : 0.77406\nEpoch : 320/500,   loss : 0.47902,    acc : 0.77273\nEpoch : 321/500,   loss : 0.47895,    acc : 0.77406\nEpoch : 322/500,   loss : 0.47891,    acc : 0.77139\nEpoch : 323/500,   loss : 0.47896,    acc : 0.77273\nEpoch : 324/500,   loss : 0.47922,    acc : 0.76872\nEpoch : 325/500,   loss : 0.48001,    acc : 0.77005\nEpoch : 326/500,   loss : 0.48188,    acc : 0.76337\nEpoch : 327/500,   loss : 0.48587,    acc : 0.77139\nEpoch : 328/500,   loss : 0.49046,    acc : 0.76738\nEpoch : 329/500,   loss : 0.49251,    acc : 0.75802\nEpoch : 330/500,   loss : 0.48561,    acc : 0.76604\nEpoch : 331/500,   loss : 0.47870,    acc : 0.77139\nEpoch : 332/500,   loss : 0.47964,    acc : 0.77005\nEpoch : 333/500,   loss : 0.48449,    acc : 0.76738\nEpoch : 334/500,   loss : 0.48428,    acc : 0.77005\nEpoch : 335/500,   loss : 0.47898,    acc : 0.76738\nEpoch : 336/500,   loss : 0.47834,    acc : 0.76872\nEpoch : 337/500,   loss : 0.48184,    acc : 0.76738\nEpoch : 338/500,   loss : 0.48130,    acc : 0.76471\nEpoch : 339/500,   loss : 0.47798,    acc : 0.77273\nEpoch : 340/500,   loss : 0.47815,    acc : 0.77406\nEpoch : 341/500,   loss : 0.48025,    acc : 0.76337\nEpoch : 342/500,   loss : 0.47932,    acc : 0.77005\nEpoch : 343/500,   loss : 0.47728,    acc : 0.77406\nEpoch : 344/500,   loss : 0.47805,    acc : 0.76872\nEpoch : 345/500,   loss : 0.47919,    acc : 0.76872\nEpoch : 346/500,   loss : 0.47793,    acc : 0.77005\nEpoch : 347/500,   loss : 0.47695,    acc : 0.77406\nEpoch : 348/500,   loss : 0.47782,    acc : 0.77273\nEpoch : 349/500,   loss : 0.47814,    acc : 0.77005\nEpoch : 350/500,   loss : 0.47712,    acc : 0.77406\nEpoch : 351/500,   loss : 0.47674,    acc : 0.77406\nEpoch : 352/500,   loss : 0.47737,    acc : 0.77005\nEpoch : 353/500,   loss : 0.47738,    acc : 0.77273\nEpoch : 354/500,   loss : 0.47664,    acc : 0.77540\nEpoch : 355/500,   loss : 0.47650,    acc : 0.77540\nEpoch : 356/500,   loss : 0.47692,    acc : 0.77139\nEpoch : 357/500,   loss : 0.47683,    acc : 0.77139\nEpoch : 358/500,   loss : 0.47633,    acc : 0.77406\nEpoch : 359/500,   loss : 0.47620,    acc : 0.77273\nEpoch : 360/500,   loss : 0.47645,    acc : 0.77139\nEpoch : 361/500,   loss : 0.47643,    acc : 0.77273\nEpoch : 362/500,   loss : 0.47609,    acc : 0.77273\nEpoch : 363/500,   loss : 0.47592,    acc : 0.77273\nEpoch : 364/500,   loss : 0.47604,    acc : 0.77406\nEpoch : 365/500,   loss : 0.47609,    acc : 0.77273\nEpoch : 366/500,   loss : 0.47589,    acc : 0.77406\nEpoch : 367/500,   loss : 0.47568,    acc : 0.77273\nEpoch : 368/500,   loss : 0.47566,    acc : 0.77540\nEpoch : 369/500,   loss : 0.47572,    acc : 0.77273\nEpoch : 370/500,   loss : 0.47567,    acc : 0.77406\nEpoch : 371/500,   loss : 0.47550,    acc : 0.77005\nEpoch : 372/500,   loss : 0.47538,    acc : 0.77139\nEpoch : 373/500,   loss : 0.47539,    acc : 0.77406\nEpoch : 374/500,   loss : 0.47543,    acc : 0.77273\nEpoch : 375/500,   loss : 0.47537,    acc : 0.77540\nEpoch : 376/500,   loss : 0.47524,    acc : 0.77005\nEpoch : 377/500,   loss : 0.47512,    acc : 0.77273\nEpoch : 378/500,   loss : 0.47508,    acc : 0.77273\nEpoch : 379/500,   loss : 0.47509,    acc : 0.77139\nEpoch : 380/500,   loss : 0.47508,    acc : 0.77674\nEpoch : 381/500,   loss : 0.47500,    acc : 0.77139\nEpoch : 382/500,   loss : 0.47490,    acc : 0.77273\nEpoch : 383/500,   loss : 0.47482,    acc : 0.77273\nEpoch : 384/500,   loss : 0.47478,    acc : 0.77273\nEpoch : 385/500,   loss : 0.47476,    acc : 0.77273\nEpoch : 386/500,   loss : 0.47474,    acc : 0.77139\nEpoch : 387/500,   loss : 0.47470,    acc : 0.77674\nEpoch : 388/500,   loss : 0.47464,    acc : 0.77005\nEpoch : 389/500,   loss : 0.47457,    acc : 0.77406\nEpoch : 390/500,   loss : 0.47450,    acc : 0.77139\nEpoch : 391/500,   loss : 0.47445,    acc : 0.77273\nEpoch : 392/500,   loss : 0.47441,    acc : 0.77406\nEpoch : 393/500,   loss : 0.47438,    acc : 0.77139\nEpoch : 394/500,   loss : 0.47436,    acc : 0.77406\nEpoch : 395/500,   loss : 0.47433,    acc : 0.77139\nEpoch : 396/500,   loss : 0.47430,    acc : 0.77674\nEpoch : 397/500,   loss : 0.47426,    acc : 0.77273\nEpoch : 398/500,   loss : 0.47422,    acc : 0.77674\nEpoch : 399/500,   loss : 0.47418,    acc : 0.77273\nEpoch : 400/500,   loss : 0.47414,    acc : 0.77674\nEpoch : 401/500,   loss : 0.47411,    acc : 0.77273\nEpoch : 402/500,   loss : 0.47408,    acc : 0.77674\nEpoch : 403/500,   loss : 0.47406,    acc : 0.77406\nEpoch : 404/500,   loss : 0.47406,    acc : 0.77941\nEpoch : 405/500,   loss : 0.47408,    acc : 0.77406\nEpoch : 406/500,   loss : 0.47415,    acc : 0.77674\nEpoch : 407/500,   loss : 0.47430,    acc : 0.77273\nEpoch : 408/500,   loss : 0.47460,    acc : 0.77941\nEpoch : 409/500,   loss : 0.47524,    acc : 0.77273\nEpoch : 410/500,   loss : 0.47644,    acc : 0.76872\nEpoch : 411/500,   loss : 0.47888,    acc : 0.77139\nEpoch : 412/500,   loss : 0.48285,    acc : 0.76872\nEpoch : 413/500,   loss : 0.48918,    acc : 0.76471\nEpoch : 414/500,   loss : 0.49401,    acc : 0.77139\nEpoch : 415/500,   loss : 0.49449,    acc : 0.75401\nEpoch : 416/500,   loss : 0.48510,    acc : 0.77139\nEpoch : 417/500,   loss : 0.47554,    acc : 0.77273\nEpoch : 418/500,   loss : 0.47437,    acc : 0.77005\nEpoch : 419/500,   loss : 0.48027,    acc : 0.76604\nEpoch : 420/500,   loss : 0.48339,    acc : 0.77273\nEpoch : 421/500,   loss : 0.47835,    acc : 0.76604\nEpoch : 422/500,   loss : 0.47369,    acc : 0.77273\nEpoch : 423/500,   loss : 0.47576,    acc : 0.77005\nEpoch : 424/500,   loss : 0.47897,    acc : 0.76738\nEpoch : 425/500,   loss : 0.47703,    acc : 0.77273\nEpoch : 426/500,   loss : 0.47368,    acc : 0.77941\nEpoch : 427/500,   loss : 0.47474,    acc : 0.78075\nEpoch : 428/500,   loss : 0.47709,    acc : 0.77406\nEpoch : 429/500,   loss : 0.47566,    acc : 0.76738\nEpoch : 430/500,   loss : 0.47349,    acc : 0.77273\nEpoch : 431/500,   loss : 0.47429,    acc : 0.77139\nEpoch : 432/500,   loss : 0.47565,    acc : 0.77139\nEpoch : 433/500,   loss : 0.47465,    acc : 0.77406\nEpoch : 434/500,   loss : 0.47334,    acc : 0.78075\nEpoch : 435/500,   loss : 0.47404,    acc : 0.77941\nEpoch : 436/500,   loss : 0.47486,    acc : 0.77406\nEpoch : 437/500,   loss : 0.47401,    acc : 0.77941\nEpoch : 438/500,   loss : 0.47323,    acc : 0.78209\nEpoch : 439/500,   loss : 0.47377,    acc : 0.76872\nEpoch : 440/500,   loss : 0.47422,    acc : 0.77941\nEpoch : 441/500,   loss : 0.47366,    acc : 0.77005\nEpoch : 442/500,   loss : 0.47314,    acc : 0.77941\nEpoch : 443/500,   loss : 0.47348,    acc : 0.77941\nEpoch : 444/500,   loss : 0.47383,    acc : 0.76872\nEpoch : 445/500,   loss : 0.47347,    acc : 0.77941\nEpoch : 446/500,   loss : 0.47306,    acc : 0.77941\nEpoch : 447/500,   loss : 0.47323,    acc : 0.77406\nEpoch : 448/500,   loss : 0.47352,    acc : 0.78075\nEpoch : 449/500,   loss : 0.47336,    acc : 0.77273\nEpoch : 450/500,   loss : 0.47301,    acc : 0.77941\nEpoch : 451/500,   loss : 0.47302,    acc : 0.78075\nEpoch : 452/500,   loss : 0.47323,    acc : 0.77139\nEpoch : 453/500,   loss : 0.47322,    acc : 0.78075\nEpoch : 454/500,   loss : 0.47300,    acc : 0.77807\nEpoch : 455/500,   loss : 0.47288,    acc : 0.78209\nEpoch : 456/500,   loss : 0.47298,    acc : 0.78075\nEpoch : 457/500,   loss : 0.47307,    acc : 0.77273\nEpoch : 458/500,   loss : 0.47297,    acc : 0.78075\nEpoch : 459/500,   loss : 0.47283,    acc : 0.77807\nEpoch : 460/500,   loss : 0.47281,    acc : 0.77941\nEpoch : 461/500,   loss : 0.47289,    acc : 0.78075\nEpoch : 462/500,   loss : 0.47291,    acc : 0.77540\nEpoch : 463/500,   loss : 0.47284,    acc : 0.78075\nEpoch : 464/500,   loss : 0.47274,    acc : 0.77807\nEpoch : 465/500,   loss : 0.47272,    acc : 0.78209\nEpoch : 466/500,   loss : 0.47275,    acc : 0.78075\nEpoch : 467/500,   loss : 0.47276,    acc : 0.77807\nEpoch : 468/500,   loss : 0.47272,    acc : 0.78209\nEpoch : 469/500,   loss : 0.47266,    acc : 0.78075\nEpoch : 470/500,   loss : 0.47263,    acc : 0.78209\nEpoch : 471/500,   loss : 0.47264,    acc : 0.78342\nEpoch : 472/500,   loss : 0.47267,    acc : 0.77807\nEpoch : 473/500,   loss : 0.47267,    acc : 0.78075\nEpoch : 474/500,   loss : 0.47264,    acc : 0.77941\nEpoch : 475/500,   loss : 0.47259,    acc : 0.78342\nEpoch : 476/500,   loss : 0.47255,    acc : 0.78075\nEpoch : 477/500,   loss : 0.47254,    acc : 0.78209\nEpoch : 478/500,   loss : 0.47255,    acc : 0.78342\nEpoch : 479/500,   loss : 0.47256,    acc : 0.77941\nEpoch : 480/500,   loss : 0.47254,    acc : 0.78342\nEpoch : 481/500,   loss : 0.47251,    acc : 0.77941\nEpoch : 482/500,   loss : 0.47248,    acc : 0.78342\nEpoch : 483/500,   loss : 0.47246,    acc : 0.78209\nEpoch : 484/500,   loss : 0.47245,    acc : 0.78209\nEpoch : 485/500,   loss : 0.47245,    acc : 0.78342\nEpoch : 486/500,   loss : 0.47245,    acc : 0.78075\nEpoch : 487/500,   loss : 0.47244,    acc : 0.78342\nEpoch : 488/500,   loss : 0.47243,    acc : 0.77941\nEpoch : 489/500,   loss : 0.47242,    acc : 0.78342\nEpoch : 490/500,   loss : 0.47240,    acc : 0.78209\nEpoch : 491/500,   loss : 0.47237,    acc : 0.78342\nEpoch : 492/500,   loss : 0.47236,    acc : 0.78209\nEpoch : 493/500,   loss : 0.47234,    acc : 0.78342\nEpoch : 494/500,   loss : 0.47233,    acc : 0.78342\nEpoch : 495/500,   loss : 0.47233,    acc : 0.78075\nEpoch : 496/500,   loss : 0.47232,    acc : 0.78342\nEpoch : 497/500,   loss : 0.47232,    acc : 0.78075\nEpoch : 498/500,   loss : 0.47233,    acc : 0.78342\nEpoch : 499/500,   loss : 0.47233,    acc : 0.77941\nEpoch : 500/500,   loss : 0.47234,    acc : 0.78342\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\n\nvalid_acc = 0\nvalid_loss = 0\nfor i in range(20) :\n    predict = model(valid_x[i])\n    \n    loss = F.binary_cross_entropy(predict, valid_y[i])\n    valid_loss += loss.item() / 20\n    \n    predict = predict >= torch.FloatTensor([0.5]).to(device)\n    predict = predict.float()\n    \n    if predict.item() == valid_y[i].item() : valid_acc += 1\n    \nprint('acc : {0}, loss : {1:.5f}'.format(valid_acc/20, valid_loss))","metadata":{"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"acc : 0.85, loss : 0.42159\n","output_type":"stream"}]}]}